"""
Feedback Loop - Performance monitoring and system improvement
"""

import asyncio
import json
import time
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
import statistics

from knowledge_base.knowledge_manager import KnowledgeManager
from utils.config import Config
from utils.logger import AgentLogger

@dataclass
class PerformanceMetric:
    """Individual performance metric"""
    name: str
    value: float
    timestamp: datetime
    component: str
    metadata: Dict[str, Any]

@dataclass
class QualityMetric:
    """Quality assessment metric"""
    prompt_id: str
    overall_score: float
    component_scores: Dict[str, float]
    user_feedback: Optional[float]
    success_indicators: Dict[str, bool]
    timestamp: datetime

@dataclass
class ImprovementSuggestion:
    """System improvement suggestion"""
    component: str
    issue: str
    suggestion: str
    priority: str  # high, medium, low
    impact: str
    implementation_effort: str

class FeedbackLoop:
    """
    Agent responsible for performance monitoring and system improvement
    
    Functions:
    1. Generated Code Quality Analysis
    2. Prompt Effectiveness Measurement  
    3. Improvement Identification
    """
    
    def __init__(self, config: Config, knowledge_manager: KnowledgeManager):
        self.config = config
        self.knowledge_manager = knowledge_manager
        self.logger = AgentLogger("FeedbackLoop")
        
        # Performance tracking
        self.performance_history: deque = deque(maxlen=1000)
        self.quality_history: deque = deque(maxlen=1000)
        self.improvement_suggestions: List[ImprovementSuggestion] = []
        
        # Monitoring windows
        self.short_term_window = timedelta(hours=1)
        self.medium_term_window = timedelta(days=1)
        self.long_term_window = timedelta(days=7)
        
        # Quality thresholds
        self.quality_thresholds = {
            "excellent": 0.9,
            "good": 0.8,
            "acceptable": 0.7,
            "poor": 0.6
        }
        
        # Performance benchmarks
        self.performance_benchmarks = {
            "dld_validation_time": 30.0,  # seconds
            "prompt_generation_time": 60.0,  # seconds
            "quality_assessment_time": 15.0,  # seconds
            "total_pipeline_time": 120.0,  # seconds
            "success_rate": 0.85  # 85% success rate target
        }
    
    async def initialize(self) -> None:
        """Initialize the feedback loop system"""
        self.logger.info("Initializing Feedback Loop system")
        
        # Load historical data
        await self._load_historical_data()
        
        # Initialize monitoring components
        await self._initialize_monitoring()
        
        # Start background monitoring tasks
        await self._start_background_monitoring()
        
        self.logger.info("Feedback Loop system initialized successfully")
    
    async def shutdown(self) -> None:
        """Shutdown the feedback loop system"""
        self.logger.info("Shutting down Feedback Loop system")
        
        # Save current data
        await self._save_monitoring_data()
        
        # Stop background tasks
        await self._stop_background_monitoring()
    
    async def analyze_performance(
        self,
        pipeline_state: Dict[str, Any],
        execution_metrics: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Main method for analyzing pipeline performance
        
        Args:
            pipeline_state: State of the processing pipeline
            execution_metrics: Metrics from pipeline execution
            
        Returns:
            Performance analysis and improvement suggestions
        """
        self.logger.info("Analyzing pipeline performance")
        
        try:\n            # Step 1: Collect Performance Metrics\n            self.logger.push_context(\"Performance Collection\")\n            performance_metrics = await self._collect_performance_metrics(\n                pipeline_state, execution_metrics\n            )\n            self.logger.pop_context()\n            \n            # Step 2: Analyze Code Quality\n            self.logger.push_context(\"Quality Analysis\")\n            quality_analysis = await self._analyze_generated_code_quality(\n                pipeline_state\n            )\n            self.logger.pop_context()\n            \n            # Step 3: Measure Prompt Effectiveness\n            self.logger.push_context(\"Effectiveness Measurement\")\n            effectiveness_metrics = await self._measure_prompt_effectiveness(\n                pipeline_state\n            )\n            self.logger.pop_context()\n            \n            # Step 4: Identify Improvement Areas\n            self.logger.push_context(\"Improvement Identification\")\n            improvement_areas = await self._identify_improvement_areas(\n                performance_metrics, quality_analysis, effectiveness_metrics\n            )\n            self.logger.pop_context()\n            \n            # Step 5: Generate Recommendations\n            self.logger.push_context(\"Recommendations\")\n            recommendations = await self._generate_improvement_recommendations(\n                improvement_areas\n            )\n            self.logger.pop_context()\n            \n            # Update historical data\n            await self._update_historical_data(performance_metrics, quality_analysis)\n            \n            # Compile results\n            analysis_result = {\n                \"performance_metrics\": performance_metrics,\n                \"quality_analysis\": quality_analysis,\n                \"effectiveness_metrics\": effectiveness_metrics,\n                \"improvement_areas\": improvement_areas,\n                \"recommendations\": recommendations,\n                \"trends\": await self._analyze_trends(),\n                \"system_health\": self._assess_system_health(performance_metrics)\n            }\n            \n            self.logger.info(\"Performance analysis completed successfully\")\n            \n            return {\n                \"success\": True,\n                \"analysis_result\": analysis_result,\n                \"summary\": self._generate_analysis_summary(analysis_result)\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Performance analysis failed: {str(e)}\")\n            return {\n                \"success\": False,\n                \"error\": str(e)\n            }\n    \n    async def _collect_performance_metrics(\n        self, \n        pipeline_state: Dict[str, Any], \n        execution_metrics: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Collect performance metrics from pipeline execution\"\"\"\n        self.logger.info(\"Collecting performance metrics\")\n        \n        metrics = {\n            \"timestamp\": datetime.now(),\n            \"execution_time\": execution_metrics.get(\"total_time\", 0),\n            \"component_times\": {},\n            \"memory_usage\": {},\n            \"success_indicators\": {},\n            \"error_counts\": {}\n        }\n        \n        # Extract component execution times\n        for component in [\"validation\", \"generation\", \"quality\", \"optimization\", \"output\"]:\n            component_result = pipeline_state.get(f\"{component}_result\", {})\n            if isinstance(component_result, dict):\n                execution_time = component_result.get(\"execution_time\", 0)\n                if execution_time > 0:\n                    metrics[\"component_times\"][component] = execution_time\n        \n        # Success indicators\n        metrics[\"success_indicators\"] = {\n            \"pipeline_completed\": pipeline_state.get(\"validation_result\", {}).get(\"success\", False),\n            \"quality_threshold_met\": pipeline_state.get(\"quality_result\", {}).get(\"quality_score\", 0) >= 0.8,\n            \"generation_successful\": pipeline_state.get(\"generation_result\", {}).get(\"success\", False),\n            \"output_generated\": bool(pipeline_state.get(\"output_result\", {}).get(\"final_prompt\"))\n        }\n        \n        # Performance ratios\n        total_time = metrics[\"execution_time\"]\n        if total_time > 0:\n            metrics[\"performance_ratios\"] = {\n                \"validation_ratio\": metrics[\"component_times\"].get(\"validation\", 0) / total_time,\n                \"generation_ratio\": metrics[\"component_times\"].get(\"generation\", 0) / total_time,\n                \"quality_ratio\": metrics[\"component_times\"].get(\"quality\", 0) / total_time\n            }\n        \n        # Store metric for historical analysis\n        perf_metric = PerformanceMetric(\n            name=\"pipeline_execution\",\n            value=total_time,\n            timestamp=metrics[\"timestamp\"],\n            component=\"master_agent\",\n            metadata=metrics\n        )\n        \n        self.performance_history.append(perf_metric)\n        \n        return metrics\n    \n    async def _analyze_generated_code_quality(\n        self, \n        pipeline_state: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Analyze the quality of generated code/prompts\"\"\"\n        self.logger.info(\"Analyzing generated code quality\")\n        \n        quality_analysis = {\n            \"overall_score\": 0.0,\n            \"component_scores\": {},\n            \"quality_indicators\": {},\n            \"improvement_areas\": []\n        }\n        \n        # Extract quality scores from pipeline\n        quality_result = pipeline_state.get(\"quality_result\", {})\n        if quality_result:\n            quality_analysis[\"overall_score\"] = quality_result.get(\"quality_score\", 0.0)\n            quality_analysis[\"component_scores\"] = quality_result.get(\"detailed_scores\", {})\n        \n        # Analyze prompt characteristics\n        final_prompt = pipeline_state.get(\"output_result\", {}).get(\"final_prompt\", \"\")\n        if final_prompt:\n            quality_analysis[\"quality_indicators\"] = self._analyze_prompt_characteristics(final_prompt)\n        \n        # Identify areas for improvement\n        for component, score in quality_analysis[\"component_scores\"].items():\n            if score < 0.7:\n                quality_analysis[\"improvement_areas\"].append({\n                    \"component\": component,\n                    \"current_score\": score,\n                    \"target_score\": 0.8,\n                    \"gap\": 0.8 - score\n                })\n        \n        # Store quality metric\n        quality_metric = QualityMetric(\n            prompt_id=f\"prompt_{int(time.time())}\",\n            overall_score=quality_analysis[\"overall_score\"],\n            component_scores=quality_analysis[\"component_scores\"],\n            user_feedback=None,  # Would be collected from users\n            success_indicators=pipeline_state.get(\"validation_result\", {}).get(\"success_indicators\", {}),\n            timestamp=datetime.now()\n        )\n        \n        self.quality_history.append(quality_metric)\n        \n        return quality_analysis\n    \n    def _analyze_prompt_characteristics(self, prompt: str) -> Dict[str, Any]:\n        \"\"\"Analyze characteristics of the generated prompt\"\"\"\n        return {\n            \"length\": len(prompt),\n            \"word_count\": len(prompt.split()),\n            \"section_count\": prompt.count(\"#\"),\n            \"has_examples\": \"example\" in prompt.lower(),\n            \"has_constraints\": \"constraint\" in prompt.lower(),\n            \"has_requirements\": \"requirement\" in prompt.lower(),\n            \"technical_density\": self._calculate_technical_density(prompt),\n            \"readability_score\": self._calculate_readability_score(prompt)\n        }\n    \n    def _calculate_technical_density(self, text: str) -> float:\n        \"\"\"Calculate technical density of the text\"\"\"\n        technical_terms = [\n            \"5G\", \"NR\", \"gNodeB\", \"AMF\", \"SMF\", \"UPF\", \"NGAP\", \"RRC\",\n            \"implement\", \"function\", \"class\", \"method\", \"algorithm\",\n            \"protocol\", \"interface\", \"API\", \"framework\"\n        ]\n        \n        word_count = len(text.split())\n        if word_count == 0:\n            return 0.0\n        \n        technical_count = sum(1 for term in technical_terms if term.lower() in text.lower())\n        return technical_count / word_count\n    \n    def _calculate_readability_score(self, text: str) -> float:\n        \"\"\"Calculate a simple readability score\"\"\"\n        sentences = text.split('.')\n        words = text.split()\n        \n        if len(sentences) == 0 or len(words) == 0:\n            return 0.0\n        \n        avg_sentence_length = len(words) / len(sentences)\n        \n        # Simple readability heuristic (lower is more readable)\n        # Ideal sentence length: 15-20 words\n        if 15 <= avg_sentence_length <= 20:\n            return 1.0\n        elif avg_sentence_length < 15:\n            return 0.8 + (avg_sentence_length / 15) * 0.2\n        else:\n            return max(0.4, 1.0 - (avg_sentence_length - 20) / 30)\n    \n    async def _measure_prompt_effectiveness(\n        self, \n        pipeline_state: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Measure the effectiveness of generated prompts\"\"\"\n        self.logger.info(\"Measuring prompt effectiveness\")\n        \n        effectiveness = {\n            \"completeness_score\": 0.0,\n            \"specificity_score\": 0.0,\n            \"actionability_score\": 0.0,\n            \"domain_relevance_score\": 0.0,\n            \"predicted_success_rate\": 0.0\n        }\n        \n        # Extract from quality assessment\n        quality_result = pipeline_state.get(\"quality_result\", {})\n        detailed_scores = quality_result.get(\"detailed_scores\", {})\n        \n        effectiveness[\"completeness_score\"] = detailed_scores.get(\"completeness\", 0.0)\n        effectiveness[\"specificity_score\"] = detailed_scores.get(\"specificity\", 0.0)\n        effectiveness[\"actionability_score\"] = detailed_scores.get(\"actionability\", 0.0)\n        \n        # Calculate domain relevance\n        validation_result = pipeline_state.get(\"validation_result\", {})\n        if validation_result:\n            effectiveness[\"domain_relevance_score\"] = validation_result.get(\"completeness_score\", 0.0)\n        \n        # Predict success rate based on historical data\n        effectiveness[\"predicted_success_rate\"] = self._predict_success_rate(effectiveness)\n        \n        return effectiveness\n    \n    def _predict_success_rate(self, effectiveness_metrics: Dict[str, float]) -> float:\n        \"\"\"Predict success rate based on effectiveness metrics\"\"\"\n        # Simple weighted prediction model\n        weights = {\n            \"completeness_score\": 0.3,\n            \"specificity_score\": 0.2,\n            \"actionability_score\": 0.3,\n            \"domain_relevance_score\": 0.2\n        }\n        \n        predicted_rate = sum(\n            effectiveness_metrics.get(metric, 0.0) * weight\n            for metric, weight in weights.items()\n        )\n        \n        return min(1.0, predicted_rate)\n    \n    async def _identify_improvement_areas(\n        self,\n        performance_metrics: Dict[str, Any],\n        quality_analysis: Dict[str, Any],\n        effectiveness_metrics: Dict[str, Any]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Identify areas for system improvement\"\"\"\n        self.logger.info(\"Identifying improvement areas\")\n        \n        improvement_areas = []\n        \n        # Performance-based improvements\n        total_time = performance_metrics.get(\"execution_time\", 0)\n        if total_time > self.performance_benchmarks[\"total_pipeline_time\"]:\n            improvement_areas.append({\n                \"area\": \"performance\",\n                \"issue\": \"Pipeline execution time exceeds benchmark\",\n                \"current_value\": total_time,\n                \"target_value\": self.performance_benchmarks[\"total_pipeline_time\"],\n                \"priority\": \"high\"\n            })\n        \n        # Quality-based improvements\n        overall_quality = quality_analysis.get(\"overall_score\", 0.0)\n        if overall_quality < self.quality_thresholds[\"good\"]:\n            improvement_areas.append({\n                \"area\": \"quality\",\n                \"issue\": \"Overall quality score below acceptable threshold\",\n                \"current_value\": overall_quality,\n                \"target_value\": self.quality_thresholds[\"good\"],\n                \"priority\": \"high\"\n            })\n        \n        # Component-specific improvements\n        component_scores = quality_analysis.get(\"component_scores\", {})\n        for component, score in component_scores.items():\n            if score < 0.7:\n                improvement_areas.append({\n                    \"area\": \"component_quality\",\n                    \"component\": component,\n                    \"issue\": f\"{component} quality score below threshold\",\n                    \"current_value\": score,\n                    \"target_value\": 0.8,\n                    \"priority\": \"medium\"\n                })\n        \n        # Effectiveness-based improvements\n        for metric, value in effectiveness_metrics.items():\n            if value < 0.7 and metric != \"predicted_success_rate\":\n                improvement_areas.append({\n                    \"area\": \"effectiveness\",\n                    \"metric\": metric,\n                    \"issue\": f\"{metric} below effectiveness threshold\",\n                    \"current_value\": value,\n                    \"target_value\": 0.8,\n                    \"priority\": \"medium\"\n                })\n        \n        return improvement_areas\n    \n    async def _generate_improvement_recommendations(\n        self, \n        improvement_areas: List[Dict[str, Any]]\n    ) -> List[ImprovementSuggestion]:\n        \"\"\"Generate specific improvement recommendations\"\"\"\n        self.logger.info(\"Generating improvement recommendations\")\n        \n        recommendations = []\n        \n        for area in improvement_areas:\n            if area[\"area\"] == \"performance\":\n                recommendations.append(ImprovementSuggestion(\n                    component=\"master_agent\",\n                    issue=\"Pipeline execution time too high\",\n                    suggestion=\"Optimize agent communication and reduce redundant processing\",\n                    priority=\"high\",\n                    impact=\"Reduced response time by 20-30%\",\n                    implementation_effort=\"medium\"\n                ))\n            \n            elif area[\"area\"] == \"quality\":\n                recommendations.append(ImprovementSuggestion(\n                    component=\"quality_agent\",\n                    issue=\"Overall quality score too low\",\n                    suggestion=\"Enhance validation rules and add more domain-specific checks\",\n                    priority=\"high\",\n                    impact=\"Improved prompt quality and user satisfaction\",\n                    implementation_effort=\"high\"\n                ))\n            \n            elif area[\"area\"] == \"component_quality\":\n                component = area.get(\"component\", \"unknown\")\n                recommendations.append(ImprovementSuggestion(\n                    component=component,\n                    issue=f\"{component} quality below threshold\",\n                    suggestion=f\"Review and enhance {component} algorithms and validation logic\",\n                    priority=\"medium\",\n                    impact=\"Improved component performance\",\n                    implementation_effort=\"medium\"\n                ))\n            \n            elif area[\"area\"] == \"effectiveness\":\n                metric = area.get(\"metric\", \"unknown\")\n                recommendations.append(ImprovementSuggestion(\n                    component=\"prompt_generator\",\n                    issue=f\"Low {metric}\",\n                    suggestion=f\"Enhance prompt templates and {metric} optimization\",\n                    priority=\"medium\",\n                    impact=\"Better prompt effectiveness\",\n                    implementation_effort=\"low\"\n                ))\n        \n        # Store recommendations\n        self.improvement_suggestions.extend(recommendations)\n        \n        return recommendations\n    \n    async def _analyze_trends(self) -> Dict[str, Any]:\n        \"\"\"Analyze performance and quality trends\"\"\"\n        trends = {\n            \"performance_trend\": \"stable\",\n            \"quality_trend\": \"stable\",\n            \"success_rate_trend\": \"stable\",\n            \"improvement_velocity\": 0.0\n        }\n        \n        if len(self.performance_history) < 5:\n            return trends\n        \n        # Analyze performance trend\n        recent_performance = [m.value for m in list(self.performance_history)[-5:]]\n        older_performance = [m.value for m in list(self.performance_history)[-10:-5]] if len(self.performance_history) >= 10 else []\n        \n        if older_performance:\n            recent_avg = statistics.mean(recent_performance)\n            older_avg = statistics.mean(older_performance)\n            \n            if recent_avg < older_avg * 0.9:\n                trends[\"performance_trend\"] = \"improving\"\n            elif recent_avg > older_avg * 1.1:\n                trends[\"performance_trend\"] = \"degrading\"\n        \n        # Analyze quality trend\n        if len(self.quality_history) >= 5:\n            recent_quality = [q.overall_score for q in list(self.quality_history)[-5:]]\n            older_quality = [q.overall_score for q in list(self.quality_history)[-10:-5]] if len(self.quality_history) >= 10 else []\n            \n            if older_quality:\n                recent_avg = statistics.mean(recent_quality)\n                older_avg = statistics.mean(older_quality)\n                \n                if recent_avg > older_avg * 1.05:\n                    trends[\"quality_trend\"] = \"improving\"\n                elif recent_avg < older_avg * 0.95:\n                    trends[\"quality_trend\"] = \"degrading\"\n        \n        return trends\n    \n    def _assess_system_health(self, performance_metrics: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Assess overall system health\"\"\"\n        health = {\n            \"status\": \"healthy\",\n            \"score\": 1.0,\n            \"issues\": [],\n            \"recommendations\": []\n        }\n        \n        # Check execution time\n        exec_time = performance_metrics.get(\"execution_time\", 0)\n        if exec_time > self.performance_benchmarks[\"total_pipeline_time\"] * 1.5:\n            health[\"status\"] = \"degraded\"\n            health[\"score\"] *= 0.7\n            health[\"issues\"].append(\"Execution time significantly above benchmark\")\n        \n        # Check success indicators\n        success_indicators = performance_metrics.get(\"success_indicators\", {})\n        success_rate = sum(success_indicators.values()) / len(success_indicators) if success_indicators else 0\n        \n        if success_rate < 0.8:\n            health[\"status\"] = \"unhealthy\"\n            health[\"score\"] *= 0.5\n            health[\"issues\"].append(\"Low success rate\")\n        \n        # Generate health recommendations\n        if health[\"issues\"]:\n            health[\"recommendations\"] = [\n                \"Review recent changes for performance impacts\",\n                \"Check system resources and dependencies\",\n                \"Validate configuration parameters\"\n            ]\n        \n        return health\n    \n    def _generate_analysis_summary(self, analysis_result: Dict[str, Any]) -> str:\n        \"\"\"Generate a human-readable summary of the analysis\"\"\"\n        performance = analysis_result[\"performance_metrics\"]\n        quality = analysis_result[\"quality_analysis\"]\n        health = analysis_result[\"system_health\"]\n        \n        summary_parts = [\n            f\"System Status: {health['status'].title()}\",\n            f\"Execution Time: {performance.get('execution_time', 0):.2f}s\",\n            f\"Quality Score: {quality.get('overall_score', 0):.2f}\",\n            f\"Health Score: {health.get('score', 0):.2f}\"\n        ]\n        \n        if analysis_result[\"improvement_areas\"]:\n            summary_parts.append(f\"Improvement Areas: {len(analysis_result['improvement_areas'])}\")\n        \n        if analysis_result[\"recommendations\"]:\n            summary_parts.append(f\"Recommendations: {len(analysis_result['recommendations'])}\")\n        \n        return \" | \".join(summary_parts)\n    \n    async def _update_historical_data(\n        self, \n        performance_metrics: Dict[str, Any], \n        quality_analysis: Dict[str, Any]\n    ) -> None:\n        \"\"\"Update historical data with new metrics\"\"\"\n        # Data is already added to deques in respective methods\n        # This method could save to persistent storage\n        pass\n    \n    async def _load_historical_data(self) -> None:\n        \"\"\"Load historical performance and quality data\"\"\"\n        self.logger.info(\"Loading historical data\")\n        # Implementation would load from persistent storage\n    \n    async def _save_monitoring_data(self) -> None:\n        \"\"\"Save monitoring data to persistent storage\"\"\"\n        self.logger.info(\"Saving monitoring data\")\n        # Implementation would save to persistent storage\n    \n    async def _initialize_monitoring(self) -> None:\n        \"\"\"Initialize monitoring components\"\"\"\n        self.logger.info(\"Initializing monitoring components\")\n    \n    async def _start_background_monitoring(self) -> None:\n        \"\"\"Start background monitoring tasks\"\"\"\n        self.logger.info(\"Starting background monitoring\")\n    \n    async def _stop_background_monitoring(self) -> None:\n        \"\"\"Stop background monitoring tasks\"\"\"\n        self.logger.info(\"Stopping background monitoring\")\n    \n    # Public methods for external access\n    \n    async def get_performance_summary(self) -> Dict[str, Any]:\n        \"\"\"Get a summary of recent performance\"\"\"\n        if not self.performance_history:\n            return {\"status\": \"no_data\", \"message\": \"No performance data available\"}\n        \n        recent_metrics = list(self.performance_history)[-10:]  # Last 10 metrics\n        \n        return {\n            \"average_execution_time\": statistics.mean([m.value for m in recent_metrics]),\n            \"min_execution_time\": min([m.value for m in recent_metrics]),\n            \"max_execution_time\": max([m.value for m in recent_metrics]),\n            \"total_executions\": len(self.performance_history),\n            \"recent_executions\": len(recent_metrics)\n        }\n    \n    async def get_quality_summary(self) -> Dict[str, Any]:\n        \"\"\"Get a summary of recent quality metrics\"\"\"\n        if not self.quality_history:\n            return {\"status\": \"no_data\", \"message\": \"No quality data available\"}\n        \n        recent_quality = list(self.quality_history)[-10:]  # Last 10 quality metrics\n        \n        return {\n            \"average_quality_score\": statistics.mean([q.overall_score for q in recent_quality]),\n            \"min_quality_score\": min([q.overall_score for q in recent_quality]),\n            \"max_quality_score\": max([q.overall_score for q in recent_quality]),\n            \"total_assessments\": len(self.quality_history),\n            \"recent_assessments\": len(recent_quality)\n        }\n    \n    async def get_improvement_suggestions(self) -> List[Dict[str, Any]]:\n        \"\"\"Get current improvement suggestions\"\"\"\n        return [asdict(suggestion) for suggestion in self.improvement_suggestions[-10:]]  # Last 10 suggestions
